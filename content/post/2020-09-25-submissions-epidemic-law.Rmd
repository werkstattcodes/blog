---
title: Submissions to Austria's amendment of epidemic law
author: roland
date: '2020-09-25'
slug: submissions_epidemic_law
categories: []
draft: true
tags:
  - Austria
  - Corona virus
  - OCR
  - stringr
  - web scraping
description: ''
---

# Setup
<details closed> 
<summary>Code: Load packages</summary> 
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(extrafont)
loadfonts(device = "win", quiet = T)
library(hrbrthemes)
hrbrthemes::update_geom_font_defaults(
  family = "Roboto Condensed",
  size = 3.5,
  color = "grey50"
)
library(scales)
library(knitr)
library(paletteer)
library(ggtext)
library(glue)
library(pdftools)
library(rvest)
library(janitor)
library(patchwork)
library(svglite)
library(countrycode)
library(tictoc)
library(furrr)
library(gt)
library(reactable)
plan(multisession, workers = 3)

```
</details>

<details closed> 
<summary>Code: Define rmarkdown options</summary> 
```{r setup, echo = T}
knit_hooks$set(wrap = function(before, options, envir) {
  if (before) {
    paste0("<", options$wrap, ">")
  } else {
    paste0("</", options$wrap, ">")
  }
})

knitr::opts_chunk$set(
 	fig.align = "left",
	message = FALSE,
	warning = FALSE,
 	dev = "svglite",
#	dev.args = list(type = "CairoPNG"),
	dpi = 300,
 	out.width = "100%"
)
options(width = 180, dplyr.width = 150)
```
</details>

<details closed> 
<summary>Code: Define plot theme, party colors, caption</summary> 
```{r echo=TRUE}

plot_bg_color <- readr::read_file(file=here::here("themes", "hugo-theme-basic", "static", "css",
                                                  "style.css")) %>% 
  str_extract(., regex("(?<=background-color:).*?(?=;)")) %>% str_trim()


theme_post <- function() {
  hrbrthemes::theme_ipsum_rc() +
    theme(
      plot.background = element_rect(fill = plot_bg_color, color=NA),
      panel.background = element_rect(fill = plot_bg_color, color=NA),
      #panel.border = element_rect(colour = plot_bg_color, fill=NA),
      #plot.border = element_rect(colour = plot_bg_color, fill=NA),
      plot.margin = margin(l = 0, 
                           t = 0.25,
                           unit = "cm"),
      plot.title = element_markdown(
        color = "grey20",
        face = "bold",
        margin = margin(l = 0, unit = "cm"),
        size = 13
      ),
      plot.title.position = "plot",
      plot.subtitle = element_text(
        color = "grey50",
        margin = margin(t = 0.2, b = 0.3, unit = "cm"),
        size = 11
      ),
      plot.caption = element_text(
        color = "grey50",
        size = 8,
        hjust = c(0)
      ),
      plot.caption.position = "panel",
      axis.title.x = element_text(
        angle = 0,
        color = "grey50",
        hjust = 1
      ),
      axis.text.x = element_text(
        size = 9,
        color = "grey50"
      ),
      axis.title.y = element_blank(),
      axis.text.y = element_text(
        size = 9,
        color = "grey50"
      ),
      panel.grid.minor.x = element_blank(),
      panel.grid.major.x = element_blank(),
      panel.grid.minor.y = element_blank(),
      panel.spacing = unit(0.25, "cm"),
      panel.spacing.y = unit(0.25, "cm"),
      strip.text = element_text(
        angle = 0,
        size = 9,
        vjust = 1,
        face = "bold"
      ),
      legend.title = element_text(
        color = "grey30",
        face = "bold",
        vjust = 1,
        size = 7
      ),
      legend.text = element_text(
        size = 7,
        color = "grey30"
      ),
      legend.justification = "left",
      legend.box = "horizontal", # arrangement of multiple legends
      legend.direction = "vertical",
      legend.margin = margin(l = 0, t = 0, unit = "cm"),
      legend.spacing.y = unit(0.07, units = "cm"),
      legend.text.align = 0,
      legend.box.just = "top",
      legend.key.height = unit(0.2, "line"),
      legend.key.width = unit(0.5, "line"),
      text = element_text(size = 5)
    )
}

data_date <- format(Sys.Date(), "%d %b %Y")

```

# Context

Austria’s government recently tabled an amendment to the country’s epidemic law. The amendment was largely a reaction to the shortcomings of the law dating from 1950 when confronted with the current Covid crisis as well as a recent ruling of Austria’s Constitutional Court which declared several measures passed by the government as violating the constitution. Hence, the government proposed the amendment (a more detailed and thorough account is available e.g. here).

While there would be a lot to say about the latent tension between civic rights and a state’s obligation to curtail an epidemic or the thin line between legitimate restrictions and excessive infringements, this post will deal with the public submissions to the government’s amendment bill. In short and crude terms, the legislative process in Austria provides the opportunity for citizens, NGOs, expert bodies etc to file submissions in which they can raise their concerns as to the (draft) version of the bill before it will be debated in parliament. So, at least in theory, it’s an avenue for the government to solicit input from the public at large. 

What caught my eye, or better my ears, was a related radio news report which mentioned something along the lines that the submissions to the amendment reached a) a record number and b) that the texts of a considerable number of submissions were similar in their wording. 

I guess the former is indicative for the fundamental issues which the amendment touches and the overall somewhat ‘edgy’, not to say polarized atmosphere when it comes to Corona. The latter point, though, the similarity of submissions' wording, puzzled me a bit. What the similarity of the wording effectively means is the use of some kind of template by those filing a submission. In this regard, the news report mentioned that there have been some pertaining calls on social media channels to oppose the bill, possibly also including the provision of a template text.  

It was this context which made me curious about the extent of the matter.At least curious enough to have an ‘empirical’ look at it with R.

So I went to the a) parliament’s website which provides a record of received submissions, b) extracted the weblinks to all those submissions of which the text is public, c) download the pertaining pdfs, d) extracted their content, and e) finally checked for their similarity. As for the last point, I can gladly report that I learned something here and my approach changed a bit as I moved along. Hence, the blog post somewhat mirrors this process. 

```{r include=FALSE}
submission_pattern <- "Ich erhebe schärfste"
```

When it comes to similarity, my first attempt was to simply look at the the recurrence of a distinct phrase which I repeatedly noticed while randomly glancing through some of the submissions. The formulation is *`r submission_pattern`* which means something like ‘to open the floodgates for arbitrariness'. What is meant is that the law's amendment is feared to open the floodgates for arbitrary rule by the government’. To put it mildly, I guess it’s safe to assume that those submissions articulated severe misgivings about the amendment. 

As I'll show below, while this approach is already somewhat informative since it reveals a pervasive use of a rather distinct phrase and hence points towards the use of text template, it is rather inductive and likely to suffer from  omissions of other, possibly even more re-current phrases. Furthermore, by simply looking at one distinct phrase is likely to be a rather 'fragile' indicator. Only a minor, effectively non-substantive modification of the wording would result in missing out on similar submissions. 

To make a long story short, this issue introduced me to the for me whole new world of quantitative text analysis as provided by the powerful `quanteda` package. Admittedly, I am only starting to scratch the surface here, but hopefully deep enough to legitimately include it in this post. So, enough of context and waffling. In media R(es).

# Get data
## Links to submission subpages

The list of submissions filed for the amendment is provided [here](https://www.parlament.gv.at/PAKT/VHG/XXVII/ME/ME_00055/index.shtml#tab-Stellungnahmen){target="_blank"} at the Parliament's website. To extract the links to the sub-pages which include the links to individual texts, I make us of the provided RSS feed. From there I extract the relevant elements and combine them to one dataframe containing the name of the person/institution filing a submission, the submission's date, and the link to the sub-page. 

<details open> 
<summary>Code: get links to submission sub-pages</summary> 
```{r eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
link_rss_all_submissions <- "https://www.parlament.gv.at/PAKT/VHG/XXVII/ME/ME_00055/filter.psp?view=RSS&jsMode=&xdocumentUri=&filterJq=&view=&GP=XXVII&ITYP=ME&INR=55&SUCH=&listeId=142&FBEZ=FP_142"

data <- xml2::read_xml(link_rss_all_submissions)

#get link to subpages with link to submissions
df_submission_pages_link <- data %>% 
  xml2::xml_find_all("//link") %>% 
  html_text() %>% 
  enframe(., 
          name="id",
          value="link_single_submission_page") %>% 
  mutate(link_single_submission_page=str_squish(link_single_submission_page)) %>% 
  filter(id>2) #removes first two rows which don't include data on submissions

#get title
df_submission_pages_name <- data %>% 
  xml2::xml_find_all("//title") %>% 
  html_text() %>% 
  enframe(., 
          name="id",
          value="title") %>% 
  mutate(name=str_extract(title, regex("(?<=\\>).*(?=\\<)"))) %>% 
  filter(id>2) %>% 
  select(-title)

#get publication date
df_submission_pages_pub_date <- data %>% 
  xml2::xml_find_all("//pubDate") %>% 
  html_text() %>% 
  enframe(., 
          name="id",
          value="date") %>% 
  mutate(date=date %>% str_squish() %>% lubridate::dmy_hms(., tz="Europe/Vienna"))

#combine to one dataframe
df_submission <- bind_cols(
  df_submission_pages_name,
  df_submission_pages_pub_date,
  df_submission_pages_link,
  ) %>% 
  select(-contains("id"))
```
</details>

```{r eval=FALSE, include=FALSE}
readr::write_excel_csv2(df_submission, path=here::here("blog_data", "epidemic_law_submissions", "df_submission_rss.csv"))
```

```{r include=FALSE}
df_submission <- readr::read_csv2(file=here::here("blog_data", "epidemic_law_submissions", "df_submission_rss.csv"))

```

As it turns out, there were `r nrow(df_submission) %>% scales::comma()` submissions to the amendment in total.

```{r echo=FALSE, paged.print=TRUE}
reactable(df_submission, 
          defaultPageSize = 4, 
          columns = list(
            link_single_submission_page = colDef(
              cell = function(value) {
                url <- value
                htmltools::tags$a(href = url, 
                                  target = "_blank", 
                                  value)
                },
              width = 150,
              html=TRUE
              )),
          filterable = TRUE,
          bordered=TRUE,
          compact = TRUE,
          style = list(fontSize = "10px"),
          theme = reactableTheme(
            backgroundColor = "#f0eff0",
                filterInputStyle = list(
                  color="green",
                  backgroundColor = "#f0eff0")))
```

Notice that there are several submissions which state 'Keine öffentliche Stellungnahme' (no public statement) as a name. Hence, we will not be able to analyse the text of all submissions, but only of those which are made public. How many are there? Let's see:

<details closed> 
<summary>Code: identify public and non-public submissions</summary> 
```{r message=FALSE, warning=FALSE}
df_submission <- df_submission %>% 
  mutate(public=case_when(str_detect(name, regex("Keine öffentliche Stellungnahme",
                                                 ignore_case = T)) ~ "not public",
                          TRUE ~ as.character("public")))

table(df_submission$public)
```
</details>

```{r echo=FALSE}
table(df_submission$public)
```

Out of `r nrow(df_submission) %>% scales::comma()` submissions, the text of `r df_submission %>% filter(public=="public") %>% nrow() %>% scales::comma()` is publicly available (no idea why some are published and others not, probably it's up to the submitter to decide). While the high number of non-public submissions is obviously quite a caveat for the further analysis, we still have the text of more than 50 % of all submissions to look into.

## Links to pdfs
Now let's use the links to the submissions' sub-pages and search on each sub-page for a link which leads to the actual pdf document of the submission. By using the browser's developer tools (F12 key), we can see the html code behind the page. Clicking on the link of one submission reveals that the link contains the word 'imfname'. I use this string to identify the link to the submission's pdf and neglect all other links and pdfs which are also on the site. 

![](/post/2020-09-25-submissions-epidemic-law_files/identify_link.gif)

To do so, below the function which is applied to all (!) links leading to the sub-pages with the individual submissions. It reads the html code, extracts the links (href), converts the results into a dataframe, and subsequently filters out the links which include 'imfname'. I then complement the extracted link with the missing part of the web address (www.parlament....). Since there are several submission (sub-pages) which state that the text is not public, and don't include a link to a pdf, I insert in these cases a 'missing' instead of the link.

<details open> 
<summary>Code: read html of sub-pages, extract pdf-link</summary> 
```{r eval=FALSE, include=TRUE}
# function to identify pdf link
fn_get_doc_links <- function(x){ 
  
  link_doc <- x %>% 
  read_html() %>% 
  html_nodes("a") %>% 
  html_attr("href") %>% 
  enframe(name=NULL,
          value="link_part") %>% 
  filter(str_detect(link_part, "imfname")) %>% 
  mutate(link_doc=paste0("https://www.parlament.gv.at", link_part)) %>% 
        pull(link_doc)
  ifelse(length(link_doc)>0, link_doc, "missing")
  
} 

#apply function to all links
df_submission <- df_submission %>% 
  mutate(link_doc=future_map_chr(link_single_submission_page, 
                          possibly(fn_get_doc_links,
                                   otherwise="missing"),
                            .progress = T))
```
</details>


```{r eval=FALSE, include=FALSE}
#check
df_submission %>% 
  filter(link_doc=="missing" & public=="public") %>% 
  nrow()
#0 = ok; only links to non-public submissions are missing;

#readr::write_excel_csv2(df_submission, path=here::here("data", "df_submission.csv"))
```

## Download submissions

Now with all pdf links available we can start to download them. To do so, I use the `walk2` function of the `purrr` package. Note that I also use `magrittr`'s  `%$%`operator to expose/pipe into the the `walk2` function.

```{r include=FALSE}
# load saved links to pdf_files
df_submission <- readr::read_csv2(file=here::here("blog_data", "epidemic_law_submissions", "df_submission.csv"))
```

<details open> 
<summary>Code: download files</summary> 
```{r eval=FALSE, include=TRUE}
#define pdf's file name and download destination
df_submission<- df_submission %>% 
  mutate(file_name=str_extract(link_doc, regex("\\d+.pdf"))) %>% 
  mutate(pdf_destination=glue::glue("{here::here('blog_data','epidemic_law_submissions','pdf_files')}/{file_name}") %>% 
           as.character() %>% 
           str_trim(., side=c("both")))

#download
df_submission %>% 
  filter(link_doc!="missing") %$% #exposes names
  walk2(
    link_doc,
    pdf_destination,
    download.file, 
    mode = "wb")
```
</details>

After the steps above we now have the all the available submission files at our disposal. Finally time to look into the actual texts.

## Extract text from pdfs
When it comes to extracting text by means of OCR, `pdftools` is the package of choice. In preparation, we need to get the German language data. Extracting text from pdfs can be quite a time consuming matter. Going through the more than 3700 submissions initially took me more than 7 hours. Reducing the dpi value from 300 to 150 accelerated the procedure quite considerably, down to approx. 60 minutes, and I didn't notice any relevant decrease of the output's quality.

<details open> 
<summary>Code: extract text from pdfs</summary> 
```{r eval=FALSE, include=TRUE}
tesseract::tesseract_download(lang = "deu")
tesseract_info()

df_submission <- df_submission %>% 
   mutate(doc_text=map(pdf_destination, possibly(~pdftools::pdf_ocr_text(.,
                                                                      language = "deu",
                                                                      dpi=72),
                                              otherwise="missing")))
```
</details>


```{r include=FALSE}
df_submission <- readr::read_rds(path=here::here("blog_data", "epidemic_law_submissions", "df_submission_150.rds")) 

```

Let's clean up the imported text, i.e. collapse multi-page submissions into one single string text, remove leading and trailing white space etc.

<details open> 
<summary>Code: clean text</summary> 
```{r}
df_submission <- df_submission %>% 
  mutate(date=date %>% 
           str_squish() %>% 
           str_trim() %>% 
           lubridate::dmy_hms()) %>% 
  mutate(doc_text=map_chr(doc_text, paste, collapse=" ")) %>% #collapse multipage docs
  mutate(doc_text=doc_text %>% str_squish() %>% str_trim(., side=c("both"))) %>% 
  mutate(name_first=str_extract(name, regex("(?<=,\\s)[:alpha:]*(?=($|[^:alpha:]|\\s))")), 
         .after="name") 
```
</details>

Let's pause for a moment and take stock: There are `r nrow(df_submission) %>% scales::comma()` submissions, out of which `r df_submission %>% filter(public=="public") %>% nrow() %>% scales::comma()` are public. From the latter we were able to extract `r df_submission %>% filter(doc_text!="missing") %>% nrow() %>% scales::comma()`. Hence, `r df_submission %>% filter(public=="public") %>% nrow() - df_submission %>% filter(doc_text!="missing") %>% nrow()` texts could not be retrieved. 

# Analysis
## Identify submissions with key formulation

Now with submissions' texts cleaned etc, let's identify those which contain the formulation which appeared quite frequently when randomly browsing through some of the texts: *`r submission_pattern`*.  

<details open> 
<summary>Code: identify submission containing key phrase</summary> 

```{r include=T, eval=F}
#submission_pattern <- "Willkür Tür und Tor"
submission_pattern <- "Ich erhebe schärfste"
```


```{r}
df_submission <- df_submission %>% 
  mutate(pattern_indicator=stringr::str_detect(doc_text,
                                                regex(pattern=submission_pattern,
                                                      dotall=T,
                                                      ignore_case=T,
                                                      multiline=T))) %>%
  mutate(overall_indicator=case_when(
      public=="not public" ~ "not public",
      pattern_indicator==TRUE ~ str_trunc(submission_pattern, 15),
      TRUE ~ as.character("other"))) %>% 
  mutate(doc_length=nchar(doc_text))
```
</details>

<details closed> 
<summary>Code: table with identified texts</summary> 
```{r}
tb_text <- reactable(df_submission %>%
            filter(pattern_indicator==TRUE) %>% 
            select(name, date, doc_text),
          defaultPageSize = 4, 
          filterable = TRUE,
          style = list(fontSize = "10px"),
          columns=list(
            name=colDef(width=100),
            date=colDef(width=100),
            doc_text=colDef(minWidth=200)),
          bordered=TRUE,
          compact = TRUE,
          theme = reactableTheme(
            backgroundColor = "#f0eff0",
                filterInputStyle = list(
                  color="green",backgroundColor = "#f0eff0")
          )
          )

```
</details>


```{r echo=FALSE}
tb_text
```

## Frequency of key formulation
<details closed> 
<summary>Code: table</summary> 
```{r message=FALSE, warning=FALSE}
# table -------------------------------------------------------------------
df_tb_submission_1 <- df_submission %>% 
  group_by(overall_indicator) %>% 
  summarise(n_abs=n()) %>% 
  mutate(n_rel=n_abs/sum(n_abs)) %>% 
  arrange(desc(n_abs)) %>% 
  mutate(indicator_tbl=case_when(overall_indicator=="not public" ~ "Keine öffentliche Stellungnahmen",
                                 overall_indicator==str_trunc(submission_pattern, 15) ~ glue::glue("enthalten Formulierung '{submission_pattern}'") %>% as.character(),
                                 overall_indicator=="other" ~ "andere öffentliche Stellungnahmen"),
         .before=1) %>% 
  select(-overall_indicator)
```

```{r message=FALSE, warning=FALSE}
df_tb_submission_2 <- df_submission %>% 
  group_by(overall_indicator) %>% 
  filter(overall_indicator!="not public") %>% 
  summarise(n_abs=n()) %>% 
  mutate(n_rel=n_abs/sum(n_abs)) %>% 
  arrange(desc(n_abs)) %>% 
  mutate(indicator_tbl=case_when(overall_indicator=="not public" ~ "Keine öffentliche Stellungnahmen",
                                 overall_indicator==str_trunc(submission_pattern, 15) ~ glue::glue("enthalten Formulierung '{submission_pattern}'") %>% as.character(),
                                 overall_indicator=="other" ~ "andere öffentliche Stellungnahmen"),
         .before=1) %>% 
  select(-overall_indicator, -n_abs, n_rel_public=n_rel)
```

```{r message=FALSE, warning=FALSE}
df_tb_submission <- df_tb_submission_1 %>% 
  left_join(., df_tb_submission_2)
```

```{r message=FALSE, warning=FALSE}
tb_submission <- df_tb_submission %>%   
  gt() %>% 
  tab_header(
    title = md("**Stellungnahmen zur Novelle des Epidemiegesetzes**"),
    subtitle = md(glue::glue(md(" Bei {df_tb_submission %>% filter(str_detect(indicator_tbl, 'Formulierung')) %>% pull(n_abs) %>% scales::comma()} Stellungnahmen von denen der Text öffentlich ist enthalten rund {df_tb_submission %>% filter(str_detect(indicator_tbl, 'Formulierung')) %>% pull(n_rel) %>% scales::percent()} die Formulierung *'{submission_pattern}'*.")))) %>% 
  tab_options(heading.align = "left",
              row_group.font.weight = "bold",
              table.background.color=plot_bg_color) %>% 
   tab_style(
    style = list(
      cell_fill(color = "orange"),
      cell_text(weight = "bold")
      ),
    locations = cells_body(
      columns = vars(n_abs),
      rows = n_abs == 2204) 
    )%>% 
    #  tab_style(
    # style = list(
    #   cell_fill(color = "orange"),
    #   cell_text(weight = "bold")
    #   ),
    # locations = cells_body(
    #   columns = vars(n_rel),
    #   rows = n_rel > 0.32 & n_rel < .34) 
    # )%>% 
       tab_style(
        style = list(
          cell_fill(color = "orange"),
          cell_text(weight = "bold")
        ),
        locations = cells_body(
        columns = vars(n_abs, n_rel, n_rel_public),
        rows = str_detect(indicator_tbl, "Formulierung")) 
    )%>% 
  cols_label(indicator_tbl="Stellungnahme",
             n_abs="Anzahl",
             n_rel="Anteil alle St.",
             n_rel_public="Anteil öff. St.") %>% 
  fmt_missing(columns=vars(n_rel_public), missing_text = "--") %>% 
  fmt_number(columns=vars(n_abs), decimals=0) %>% 
  fmt_percent(columns = vars(n_rel, n_rel_public), decimals = 1) %>% 
  tab_source_note(source_note = md("data: www.parlament.gv.at, 'Ministerialentwurf betreffend Bundesgesetz, mit dem das Epidemiegesetz 1950, das Tuberkulosegesetz und das COVID-19-Maßnahmengesetz geändert werden.'<br>analysis: Roland Schmidt | @zoowalk | https://werk.statt.codes"))

```
</details>

As the table below shows, in `r df_tb_submission %>% filter(str_detect(indicator_tbl, "Formulierung")) %>% pull(n_abs) %>% scales::comma()` submissions the formulation '*`r submission_pattern`*'  appears. *This number amounts to  `r df_tb_submission %>% filter(str_detect(indicator_tbl, "Formulierung")) %>% pull(n_rel) %>% scales::percent()` of all submissions, and to `r (df_tb_submission %>% filter(str_detect(indicator_tbl, "Formulierung")) %>% pull(n_abs)/(df_tb_submission %>% filter(str_detect(indicator_tbl, "Formulierung")) %>% pull(n_abs)+df_tb_submission %>% filter(str_detect(indicator_tbl, "andere")) %>% pull(n_abs))) %>% scales::percent()` of all submissions for which we can access the text.* I think that's quite remarkable.


```{r echo=FALSE, message=FALSE, warning=FALSE}
tb_submission

```


## Distribution of length
Let's approach the similarity of submissions from another angle now: The length of the text. The graph below shows the distribution of submissions' length in terms of number of characters. One density curve shows the distribution for submissions which include the key phrase '`r str_trunc(submission_pattern, 20)`', the other one shows the distribution of those which don't include it (and for which the text is available).

<details closed> 
<summary>Code: distribution of length</summary> 
```{r message=FALSE, warning=FALSE}

# distribution of length --------------------------------------------------

my_caption <- glue::glue("data: www.parlament.gv.at; Stellungnahmen zum Ministerialentwurf betreffend Bundesgesetz, mit dem das Epidemiegesetz 1950, das Tuberkulosegesetz \nund das COVID-19-Maßnahmengesetz geändert werden.\nanalysis: Roland Schmidt | @zoowalk | https://werk.statt.codes")

pl_length <- df_submission %>% 
  filter(public!="not public") %>%
  filter(doc_length<1000) %>% #remove outliers
  ggplot(., aes(y=pattern_indicator,
             x=doc_length,
             fill=pattern_indicator,
             color=pattern_indicator)
         )+
  labs(title=paste("BEGUTACHTUNGSVERFAHREN EPIDEMIEGESETZ-NOVELLE:<br>","Verteilung der Länge von Stellungnahmen mit u. ohne Formulierung ","'",str_trunc(submission_pattern, 20),"'"),
       subtitle="Zur besseren Sichtbarkeit wurden Stellungnahmen mit mehr als 1000 Zeichen wurden ausgeschlossen.",
       x="Länge Stellungnahme in Zeichen",
       caption=my_caption)+
  ggridges::geom_density_ridges(scale=5)+
  scale_fill_manual(values=c("FALSE"="darkolivegreen4",
                                   "TRUE"="orange"),
                    labels=c("FALSE"=glue::glue("ohne '{str_trunc(submission_pattern, 20)}'"),
                             "TRUE"=glue::glue("mit '{str_trunc(submission_pattern, 20)}'")))+
  scale_color_manual(values=c("FALSE"="darkolivegreen4",
                             "TRUE"="orange"),
                     labels=c("FALSE"=glue::glue("ohne '{str_trunc(submission_pattern, 20)}'"),
                             "TRUE"=glue::glue("mit '{str_trunc(submission_pattern, 20)}'")))+
  scale_y_discrete(expand=expansion(mult=c(0, 0.1)))+
  theme_post()+
  theme(
    plot.title.position = "plot",
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    plot.caption = element_text(hjust=0),
    legend.position = "top",
    legend.justification = "left",
   legend.direction = "horizontal",
    legend.title=element_blank(),
    axis.title.y = element_blank(),
    axis.text.y=element_blank())+
  guides(fill=guide_legend(reverse=T),
         color=guide_legend(reverse=T))
  
```
</details>

```{r echo=FALSE, dev='svg'}
pl_length
```

What the graph shows is that the length of submissions which contain the formulation '`r str_trunc(submission_pattern, 25)`' is concentrated around 350 characters, while the length of other submissions is more dispersed. So, when it comes to the length of documents, those containing the distinct formulation are also pretty similar in term of their length. I take this as another indicator for the similarity of the submissions with our search phrase. As for the density curve of those not containing the search phrase, note that there is still a little 'bump' around 300. This could potentially reflect that we are missing out on some submissions which may originate from the same submission template.

## Submissions over time
I was also wondering whether there is any 'temporal' pattern as to the date of the submission. If the submissions were the result of a social media campaign, are the timings of their submissions clustered? This could be read as a reaction to e.g. a social media post providing a template and asking to submit it.

<details closed> 
<summary>Code: submissions per day</summary> 
```{r message=FALSE, warning=FALSE}
# bar graph ---------------------------------------------------------------

df_submission_pattern <- df_submission %>% 
  mutate(overall_indicator=fct_infreq(overall_indicator)) %>% 
  group_by(date, overall_indicator, .drop=F) %>% 
  summarise(n=n()) %>% 
  ungroup() %>% 
  mutate(date=as.Date(date))

levels(df_submission_pattern$overall_indicator)

labe_submission_pattern <- as.character(glue::glue("{str_trunc(submission_pattern,15)}"))

colors_indicators=c("grey70", "orange", "darkolivegreen4")
names(colors_indicators) <- c("not public", str_trunc(submission_pattern, 15), "other")

pl_submission_pattern <- df_submission_pattern %>% 
  filter(date<=as.Date("2020-09-19")) %>% 
  ggplot()+
  labs(title=paste("BEGUTACHTUNGSVERFAHREN EPIDEMIEGESETZ-NOVELLE:<br>","Zeitpunkt der Einrechung der Stellungnahmen, mit u. ohne Formulierung"),
       subtitle="",
       caption=my_caption,
       x="Datum",
       y="Anzahl Einreichungen")+
  geom_bar(aes(x=date,
               y=n,
               fill=overall_indicator),
           position=position_dodge2(),
           color=NA,
           stat="identity",
           key_glyph = "dotplot")+
  scale_x_date(labels=scales::label_date_short(),
               date_breaks="1 days")+
  scale_y_continuous(expand=expansion(mult=c(0,0.1)),
                     labels = scales::label_comma())+
  scale_fill_manual(values=colors_indicators,
                    labels=c("not public"="keine öffentliche Stellungnahme",
                             label_submission_pattern=glue::glue("enthält Formulierung {str_trunc(submission_pattern, 30)}"),
                                  "other"="andere öffentliche Stellungnahmen"))+
  theme_post()+
  theme(
    plot.title.position = "panel",
    plot.subtitle = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    plot.caption = element_text(hjust=0),
    legend.position = "top",
    legend.box.margin = margin(l=0, unit="cm"),
    legend.margin = margin(l=0, unit="cm"),
    legend.key.size = unit(1, units="cm"),
    legend.justification = "left",
    legend.title=element_blank(),
    axis.title.x = element_blank()
  )+
  guides(fill=guide_legend(size=15))
```
</details>


```{r echo=FALSE, dev='svg'}
pl_submission_pattern
```

I think the bar chart above doesn't suggest any temporal clustering of submissions including '`r submission_pattern`'. But considering that the speed with which the amendment was introduced and the very short period to submissions doesn't really allow for much clustering. 


# Analysis with 'quanteda'
As already indicated in the introduction, the above analysis - while informative to some extent - has its limitations. Limiting the search to '`r submission_pattern`' is prone to miss out on other wordings which might be even more prevalent in some of the submissions. Furthermore, only marginal changes to the wording could result in the omission of submissions which are otherwise very similar.

To overcome these limitations and approach the issue of text similarity in a methodologically more rigors manner, let's now take up some of the features of the `quanteda` package. I never worked with the package before but from what I can tell it provides a very comprehensive and powerful set of tools for quantitative text analysis (for an overview see [here](https://quanteda.io/articles/pkgdown/comparison.html){target="_blank"}). 
To be able to use the offered tools, we have to convert our collection of submission texts into a 'corpus'. Before doing so, I further clean  the extracted texts so that we only have the actual text of the submission and not any header, footer, or meta-data. Then let's create a `corpus`.

<details open> 
<summary>Code: Clean text, create corpus</summary> 
```{r}
library(quanteda)
df_submission_quanteda <- df_submission %>% 
  filter(doc_text!="missing") %>% 
  select(file_name, doc_text) %>% 
  mutate(doc_text=str_remove(doc_text,regex("^.*Eingebracht am: \\d{2}\\.\\d{2}\\.\\d{4}\\s*"))) %>% 
  mutate(doc_text=str_remove(doc_text,regex("\\s?www.parlament.gv.at\\s*$"))) %>% 
  mutate(doc_text=str_remove(doc_text,regex("^.?Betr\\.?\\:?\\s?Änderung(en)? des Ep(i|e)demie Gesetzes\\.?"))) %>% 
  mutate(doc_text=doc_text %>% str_squish() %>% str_trim(., side=c("both"))) %>% 
  ungroup()

#create vector
vec_submission <- df_submission_quanteda %>% 
  deframe()

#create corpus
corp_submission <- corpus(vec_submission, docvars=data.frame(submission=names(vec_submission)))
```
</details> 

The corpus is essentially a named vector.
```{r echo=FALSE}
head(corp_submission)
```

## Collocations
With this as the basis, we can now identify all collocations e.g. which comprise `r stringi::stri_count_words(submission_pattern)-1` to `r stringi::stri_count_words(submission_pattern)` words as e.g. in `r submission_pattern`). The `textstat_collocation` function provides us with  the frequencies for all (!) collocations of the requested length (For speed reasons I limited the results to those collocations which appear in more than 500 hits). For a detailed description of the function see [here](https://quanteda.io/reference/textstat_collocations.html){target="_blank"}.

<details open> 
<summary>Code: Get collocations </summary> 
```{r}
#collocations

n_words <- str_count(submission_pattern, regex("\\S+"))
n_words

collocs <- textstat_collocations(
  corp_submission,
  method = "lambda",
  size = (n_words-1):n_words,
  min_count = 500,
  smoothing = 0.5,
  tolower = T) %>% 
  arrange(desc(count))

df_collocs <- collocs %>% 
  as_tibble()

```
</details> 

And as it turns out, our used `r submission_pattern` is close to the most frequent one. The collocation `r df_collocs %>% slice_head(n=1) %>% pull(collocation)` appears `r df_collocs %>% slice_head(n=1) %>% pull(count) %>% scales::comma()`.^[Note that the frequency of `r submission_pattern` quanteda result is different than the frequency I detected earlier. I am not entirely sure why this is, but I assume it's due to different approaches. While our result above is the actual]

```{r echo=FALSE}
reactable(df_collocs,
          bordered=TRUE,
          compact = TRUE,
          style = list(fontSize = "10px"),
          filterable = TRUE,
          theme = reactableTheme(
            backgroundColor = "#f0eff0",
                filterInputStyle = list(
                  color="green",
                  backgroundColor = "#f0eff0")))
```

## Cosine similarity

In order to control for different document length when it comes to check for similarity, we have to create a weighted document feature matrix which accounts for the relative frequency of words (see [here](https://quanteda.io/reference/dfm_weight.html?q=dfm%20_%20weight){target="_blank"}.

<details open> 
<summary>Code: create document feature matrix (dfm) </summary> 
```{r}
#create document-feature matrix
dfmat_submission <- dfm(corp_submission, 
                        remove = stopwords("de"),
                        stem = TRUE, 
                        remove_punct = TRUE)

dfmat_prop_submission <- dfm_weight(dfmat_submission,
                                    scheme="prop")
```
</details>


With this we can calculate the cosine similarity of texts. 

<details open> 
<summary>Code: create similarity matrix (dfm) </summary> 
```{r}
#create similarity matrix
mat_similarity <- textstat_simil(dfmat_prop_submission, dfmat_prop_submission, 
                            margin = "documents", 
                            method = "cosine")

#convert to dataframe
df_similarity <- mat_similarity %>% 
  as.data.frame(., 
                upper=FALSE,
                diag=FALSE,
  ) %>% 
  mutate(sim_interval=cut(cosine, c(seq(0,1,.1))))
```
</details>


```{r echo=FALSE}
reactable(head(df_similarity, n = 10^3),
          bordered=TRUE,
          compact = TRUE,
          style = list(fontSize = "10px"),
          filterable = TRUE,
          theme = reactableTheme(
            backgroundColor = "#f0eff0",
                filterInputStyle = list(
                  color="green",
                  backgroundColor = "#f0eff0")))

```

What we have obtained now is a comparison of all document pairs as to their cosine similarity. Overall, these are `r df_similarity %>% nrow() %>% scales::comma()` pairs. Seems like text analysis can quickly get a bit large. To keep the table manageable I only show the first thousand).

Note that the numbers do not represent e.g. % of similar words, but are 

A rather intuitive explanation of how similarity is conceptualized and measured under this approach have a look at [this video](https://youtu.be/7cwBhWYHgsA?t=193){target="_blank"}. 

To be able to group these pairs according to their similarity, I cut their cosine values into 10 intervals (`sim_interval`). We can now count the number of document pairs per interval. The barplot below presents the result.

<details open> 
<summary>Code: Frequency of pairs per cosine interval </summary> 
```{r}
#barplot
df_bar_sim <- df_similarity %>% 
  group_by(sim_interval) %>% 
  summarise(interval_n=n()) %>% 
  mutate(interval_rel=interval_n/sum(interval_n)) %>% 
  ungroup() %>% 
  mutate(law="EpidemieG")

pl_bar_sim <- df_bar_sim %>% 
  ggplot() +
  labs(
    title="Similarity of submissions",
    subtitle="",
    y="% of all document pairs",
    x="cosine similarity",
    caption=my_caption)+
  geom_bar(aes(x=sim_interval,
               y=interval_rel),
           fill="#374E55",
           stat="identity")+
    scale_y_continuous(labels=scales::label_percent())+
  theme_post()+
  theme(axis.title.y = element_text(angle = 90,
                                    size=9,
                                    color="grey50",
                                    hjust=1))

```
</details>

```{r echo=FALSE, dev='svg'}
pl_bar_sim
```

What we see here is that `r df_bar_sim %>% slice_max(sim_interval, n=2) %>% pull(interval_rel) %>% sum() %>% scales::percent()` all document pairs fall into a cosine similarity above 0.8, and are hence rather similar. 

### Groupwise comparison
The plot, however, does not provide any information on whether the submissions with the phrase '`r submission_pattern`' were indeed overall more similar to each other than to other submissions. To check for this I add indicators to the pairwise comparison, showing whether both, only one, or none of the two compared documents were including the key phrase. Subsequently, we can contrast the distribution of the cosine values between those three groups.


```{r}
#create indicator for each document
df_submission <- df_submission %>%
  mutate(search_term=str_detect(doc_text, regex(submission_pattern,
                                                ignore_case = T,
                                                multiline = T,
                                                dotall = T))) 

#add indicator to pairwise similarity comparison for first document
df_similarity <- df_similarity %>% 
  left_join(.,
            df_submission %>% 
              select(file_name, search_term),
            by=c("document1"="file_name")) %>% 
  rename(document1_term=search_term)

#add indicator to pairwise similarity comparison for first document
df_similarity <- df_similarity %>% 
  left_join(.,
            df_submission %>% 
              select(file_name, search_term),
            by=c("document2"="file_name")) %>% 
  rename(document2_term=search_term)

#create one, both, none groups
df_similarity <- df_similarity %>% 
  mutate(group_id=case_when(
    document1_term==TRUE & document2_term==TRUE ~ "both",
    document1_term==FALSE & document2_term==FALSE ~ "none",
    document1_term==TRUE & document2_term==FALSE ~ "one",
    document1_term==FALSE & document2_term==TRUE ~ "one",
    TRUE ~ as.character("missing")))

pl_sim_groups <- df_similarity %>% 
  ggplot()+
  labs(
    title="Similarity of ",
    subtitle=glue::glue("Grouped by presence/absence of {submission_pattern}"),
    y="cosine similarity",
       caption=my_caption)+
  geom_boxplot(aes(x=group_id,
                   y=cosine,
                   color=group_id),
               fill=NA)+
  # geom_jitter(aes(x=group_id,
  #                 y=cosine,
  #                 color=group_id))+
  theme_post()+
  theme(legend.position = "none")+
  scale_color_paletteer_d("ggsci::default_jama")
    

```

```{r, dev='CairoPNG', dpi=300, echo=FALSE}
pl_sim_groups
```

I think the result shows quite clearly that those submissions which include the  key phrase are much more similar to each other than all other groups of document pairs. Clearly, the highlighted similarity goes beyond the mere presence of the the three words of '`r submission_pattern`'. The graph also shows that those submission which do not contain the wording are hardly similar to each other.  

## Comparision
But how does this result contrast with e.g. submissions to other bills. To put the results for the epidemic bill into context, I repeated the analysis from above for another bill, the 2018/19 Fundamental Law on Social Welfare ('[Sozialhilfegrundgesetz](https://www.parlament.gv.at/PAKT/VHG/XXVI/ME/ME_00104/index.shtml#tab-Stellungnahmen){target="_blank"}').^[I don't reproduce the code for the analysis here, but it is available on my github.]

```{r echo=FALSE}
df_sozial_sim <- readr::read_csv2(file=here::here("blog_data", "epidemic_law_submissions", "df_bar_sim_sozial.csv")) %>% 
  mutate(law="Sozialhilfegrundgesetz")

df_sim_comb <- bind_rows(df_sozial_sim, df_bar_sim)

pl_sim <- df_sim_comb %>% 
  ggplot() +
  labs(title="Epidemiegesetz Übungen <span style=color:'red'>Cosine</span> Similarity von Stellungnahmen",
    y="% of all submission pairs",
    x="cosine similarity",
    caption=my_caption)+
  geom_bar(aes(x=sim_interval,
               fill=law,
               y=interval_rel),
           position=position_dodge2(),
           stat="identity")+
  scale_y_continuous(labels=scales::label_percent())+
  scale_fill_paletteer_d("ggsci::default_jama")  +
  theme_post()+
  theme(axis.title.y = element_text(angle = 90,
                                    size=9,
                                    color="grey50",
                                    hjust=1),
        plot.title.position = "panel",
        legend.position = "top",
        legend.direction = "horizontal",
        legend.title = element_blank())
```

```{r echo=FALSE, dev='svg'}
pl_sim
```

# Wrap up
So that's it (for now). Again, this got more lengthy than initially intended. A next step could be to see whether the similarity to submissions grew since the ability to file submissions via email. After all, copying a template, modifying it a bit before submission is not too difficult. Maybe as a final word, this is not to say that copy-and-paste submissions are of some sort of diminished democratic value. 




